{{ $componentName := printf "%s-cm" .Chart.Name }}
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ $componentName }}
  {{- if .Values.ownerReference.overRide }}
  {{- with  .Values.ownerReference.ownerReferences }}
  ownerReferences: {{- toYaml . | nindent 2 }}
  {{- end }}
  {{- end }}
  labels:
  {{- include "common.labels" (dict "componentName" $componentName "namespace" .Release.Namespace ) | nindent 4 }}
data:
  storageKind: "{{- print .Values.eventlogstorage.kind }}"
  pvcName: "{{- print .Values.eventlogstorage.pvcName }}"
  dep-blacklist.txt: ""
  pre-startup.sh: |
    #!/usr/bin/env bash
  post-startup.sh: |
    #!/usr/bin/env bash
  log4j.properties: |-
    #
    # Licensed to the Apache Software Foundation (ASF) under one or more
    # contributor license agreements.  See the NOTICE file distributed with
    # this work for additional information regarding copyright ownership.
    # The ASF licenses this file to You under the Apache License, Version 2.0
    # (the "License"); you may not use this file except in compliance with
    # the License.  You may obtain a copy of the License at
    #
    #    http://www.apache.org/licenses/LICENSE-2.0
    #
    # Unless required by applicable law or agreed to in writing, software
    # distributed under the License is distributed on an "AS IS" BASIS,
    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    # See the License for the specific language governing permissions and
    # limitations under the License.
    #

    # Set everything to be logged to the console
    log4j.rootCategory=WARN, console
    log4j.appender.console=org.apache.log4j.ConsoleAppender
    log4j.appender.console.target=System.err
    log4j.appender.console.layout=org.apache.log4j.PatternLayout
    log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n

    # Set the default spark-shell log level to WARN. When running the spark-shell, the
    # log level for this class is used to overwrite the root logger's log level, so that
    # the user can have different defaults for the shell and regular Spark apps.
    log4j.logger.org.apache.spark.repl.Main=WARN

    # Settings to quiet third party logs that are too verbose
    log4j.logger.org.spark_project.jetty=WARN
    log4j.logger.org.spark_project.jetty.util.component.AbstractLifeCycle=ERROR
    log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO
    log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO
    log4j.logger.org.apache.parquet=ERROR
    log4j.logger.parquet=ERROR

    # SPARK-9183: Settings to avoid annoying messages when looking up nonexistent UDFs in SparkSQL with Hive support
    log4j.logger.org.apache.hadoop.hive.metastore.RetryingHMSHandler=FATAL
    log4j.logger.org.apache.hadoop.hive.ql.exec.FunctionRegistry=ERROR
    log4j.logger.org.apache.hadoop.hive.conf.HiveConf=ERROR

    # SPARK-327: Settings to suppress the unnecessary warning message from MultiMechsAuthenticationHandler
    log4j.logger.org.apache.hadoop.security.authentication.server.MultiMechsAuthenticationHandler=ERROR
    log4j.logger.org.apache.hadoop.security.authentication.server.KerberosAuthHandler=ERROR
  metrics.properties: |
    #
    # Licensed to the Apache Software Foundation (ASF) under one or more
    # contributor license agreements.  See the NOTICE file distributed with
    # this work for additional information regarding copyright ownership.
    # The ASF licenses this file to You under the Apache License, Version 2.0
    # (the "License"); you may not use this file except in compliance with
    # the License.  You may obtain a copy of the License at
    #
    #    http://www.apache.org/licenses/LICENSE-2.0
    #
    # Unless required by applicable law or agreed to in writing, software
    # distributed under the License is distributed on an "AS IS" BASIS,
    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    # See the License for the specific language governing permissions and
    # limitations under the License.
    #

    #  syntax: [instance].sink|source.[name].[options]=[value]

    #  This file configures Spark's internal metrics system. The metrics system is
    #  divided into instances which correspond to internal components.
    #  Each instance can be configured to report its metrics to one or more sinks.
    #  Accepted values for [instance] are "master", "worker", "executor", "driver",
    #  and "applications". A wildcard "*" can be used as an instance name, in
    #  which case all instances will inherit the supplied property.
    #
    #  Within an instance, a "source" specifies a particular set of grouped metrics.
    #  there are two kinds of sources:
    #    1. Spark internal sources, like MasterSource, WorkerSource, etc, which will
    #    collect a Spark component's internal state. Each instance is paired with a
    #    Spark source that is added automatically.
    #    2. Common sources, like JvmSource, which will collect low level state.
    #    These can be added through configuration options and are then loaded
    #    using reflection.
    #
    #  A "sink" specifies where metrics are delivered to. Each instance can be
    #  assigned one or more sinks.
    #
    #  The sink|source field specifies whether the property relates to a sink or
    #  source.
    #
    #  The [name] field specifies the name of source or sink.
    #
    #  The [options] field is the specific property of this source or sink. The
    #  source or sink is responsible for parsing this property.
    #
    #  Notes:
    #    1. To add a new sink, set the "class" option to a fully qualified class
    #    name (see examples below).
    #    2. Some sinks involve a polling period. The minimum allowed polling period
    #    is 1 second.
    #    3. Wildcard properties can be overridden by more specific properties.
    #    For example, master.sink.console.period takes precedence over
    #    *.sink.console.period.
    #    4. A metrics specific configuration
    #    "spark.metrics.conf=${SPARK_HOME}/conf/metrics.properties" should be
    #    added to Java properties using -Dspark.metrics.conf=xxx if you want to
    #    customize metrics system. You can also put the file in ${SPARK_HOME}/conf
    #    and it will be loaded automatically.
    #    5. The MetricsServlet sink is added by default as a sink in the master,
    #    worker and driver, and you can send HTTP requests to the "/metrics/json"
    #    endpoint to get a snapshot of all the registered metrics in JSON format.
    #    For master, requests to the "/metrics/master/json" and
    #    "/metrics/applications/json" endpoints can be sent separately to get
    #    metrics snapshots of the master instance and applications. This
    #    MetricsServlet does not have to be configured.

    ## List of available common sources and their properties.

    # org.apache.spark.metrics.source.JvmSource
    #   Note: Currently, JvmSource is the only available common source.
    #         It can be added to an instance by setting the "class" option to its
    #         fully qualified class name (see examples below).

    ## List of available sinks and their properties.

    # org.apache.spark.metrics.sink.ConsoleSink
    #   Name:   Default:   Description:
    #   period  10         Poll period
    #   unit    seconds    Unit of the poll period

    # org.apache.spark.metrics.sink.CSVSink
    #   Name:     Default:   Description:
    #   period    10         Poll period
    #   unit      seconds    Unit of the poll period
    #   directory /tmp       Where to store CSV files

    # org.apache.spark.metrics.sink.GangliaSink
    #   Name:     Default:   Description:
    #   host      NONE       Hostname or multicast group of the Ganglia server,
    #                        must be set
    #   port      NONE       Port of the Ganglia server(s), must be set
    #   period    10         Poll period
    #   unit      seconds    Unit of the poll period
    #   ttl       1          TTL of messages sent by Ganglia
    #   dmax      0          Lifetime in seconds of metrics (0 never expired)
    #   mode      multicast  Ganglia network mode ('unicast' or 'multicast')

    # org.apache.spark.metrics.sink.JmxSink

    # org.apache.spark.metrics.sink.MetricsServlet
    #   Name:     Default:   Description:
    #   path      VARIES*    Path prefix from the web server root
    #   sample    false      Whether to show entire set of samples for histograms
    #                        ('false' or 'true')
    #
    # * Default path is /metrics/json for all instances except the master. The
    #   master has two paths:
    #     /metrics/applications/json # App information
    #     /metrics/master/json       # Master information

    # org.apache.spark.metrics.sink.GraphiteSink
    #   Name:     Default:      Description:
    #   host      NONE          Hostname of the Graphite server, must be set
    #   port      NONE          Port of the Graphite server, must be set
    #   period    10            Poll period
    #   unit      seconds       Unit of the poll period
    #   prefix    EMPTY STRING  Prefix to prepend to every metric's name
    #   protocol  tcp           Protocol ("tcp" or "udp") to use

    # org.apache.spark.metrics.sink.StatsdSink
    #   Name:     Default:      Description:
    #   host      127.0.0.1     Hostname or IP of StatsD server
    #   port      8125          Port of StatsD server
    #   period    10            Poll period
    #   unit      seconds       Units of poll period
    #   prefix    EMPTY STRING  Prefix to prepend to metric name

    ## Examples
    # Enable JmxSink for all instances by class name
    *.sink.jmx.class=org.apache.spark.metrics.sink.JmxSink

    # Enable ConsoleSink for all instances by class name
    #*.sink.console.class=org.apache.spark.metrics.sink.ConsoleSink

    # Enable StatsdSink for all instances by class name
    #*.sink.statsd.class=org.apache.spark.metrics.sink.StatsdSink
    #*.sink.statsd.prefix=spark

    # Polling period for the ConsoleSink
    #*.sink.console.period=10
    # Unit of the polling period for the ConsoleSink
    #*.sink.console.unit=seconds

    # Polling period for the ConsoleSink specific for the master instance
    #master.sink.console.period=15
    # Unit of the polling period for the ConsoleSink specific for the master
    # instance
    #master.sink.console.unit=seconds

    # Enable CsvSink for all instances by class name
    #*.sink.csv.class=org.apache.spark.metrics.sink.CsvSink

    # Polling period for the CsvSink
    #*.sink.csv.period=1
    # Unit of the polling period for the CsvSink
    #*.sink.csv.unit=minutes

    # Polling directory for CsvSink
    #*.sink.csv.directory=/tmp/

    # Polling period for the CsvSink specific for the worker instance
    #worker.sink.csv.period=10
    # Unit of the polling period for the CsvSink specific for the worker instance
    #worker.sink.csv.unit=minutes

    # Enable Slf4jSink for all instances by class name
    #*.sink.slf4j.class=org.apache.spark.metrics.sink.Slf4jSink

    # Polling period for the Slf4JSink
    #*.sink.slf4j.period=1
    # Unit of the polling period for the Slf4jSink
    #*.sink.slf4j.unit=minutes

    # Enable JvmSource for instance master, worker, driver and executor
    master.source.jvm.class=org.apache.spark.metrics.source.JvmSource

    worker.source.jvm.class=org.apache.spark.metrics.source.JvmSource

    driver.source.jvm.class=org.apache.spark.metrics.source.JvmSource

    executor.source.jvm.class=org.apache.spark.metrics.source.JvmSource
  spark-defaults.conf: |
    # Default system properties included when running spark-submit.
    # This is useful for setting default environmental settings.

    # Log effective Spark configuration at startup on INFO level
    spark.logConf                      true

    # Enable event logs for HistoryServer
    spark.eventLog.enabled             true

    # Default location for Warehouse, if not using Hive
    spark.sql.warehouse.dir            maprfs:///user/${system:user.name}/spark-warehouse

    # Fix for SPARK-7819
    spark.sql.hive.metastore.sharedPrefixes  com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,oracle.jdbc,com.mapr.fs.shim.LibraryLoader,com.mapr.security.JNISecurity,com.mapr.fs.jni,com.mapr.fs.ShimLoader

    spark.executor.memory              2g

    spark.yarn.historyServer.address spark-hs-container-d46d4fd9d-zvnlf.spark-hs-container-svc.internaltenant.svc.cluster.local:18480
    spark.history.ui.port {{ .Values.ports.httpPort }}

    # - PAM
    spark.ui.filters  org.apache.spark.ui.filters.MultiauthWebUiFilter

    {{- if not .Values.tenantIsUnsecure }}
    # SECURITY BLOCK
    # ALL SECURITY PROPERTIES MUST BE PLACED IN THIS BLOCK

    # ssl
    spark.ssl.historyServer.port {{ .Values.ports.httpsPort }}
    spark.ssl.enabled true
    spark.ssl.fs.enabled true
    spark.ssl.protocol TLSv1.2

    # - ACLS
    spark.acls.enable       false
    spark.admin.acls        mapr
    spark.admin.acls.groups mapr
    spark.ui.view.acls      mapruser1

    # - Authorization and Network Encryption
    spark.authenticate      true
    # - - This secret will be used only by local/standalone modes. YARN will override this with its own secret
    spark.authenticate.secret       changeMe
    spark.authenticate.enableSaslEncryption true
    spark.network.sasl.serverAlwaysEncrypt  true
    # - IO Encryption
    spark.io.encryption.enabled     true
    spark.io.encryption.keySizeBits 128

    # END OF THE SECURITY CONFIGURATION BLOCK
    {{- end }}


    spark.eventLog.dir {{ include "spark-hs-chart.eventLogPath" . }}
    spark.history.fs.logDirectory {{ include "spark-hs-chart.eventLogPath" . }}

    {{- if ( eq .Values.eventlogstorage.kind "s3") }}
    spark.hadoop.fs.s3a.endpoint:   {{ .Values.eventlogstorage.s3Endpoint }}
    spark.history.fs.logDirectory   {{ .Values.eventlogstorage.s3path }}
    spark.hadoop.fs.s3a.impl        org.apache.hadoop.fs.s3a.S3AFileSystem
    {{- end }}
  spark-jars.sh: |
    #!/bin/bash

    SPARK_INSTALLATION_DIRECTORY="$( cd "$( dirname "${BASH_SOURCE[0]}" )"/.. && pwd )"
    SPARK_JARS_CLASSPATH=$(find $SPARK_INSTALLATION_DIRECTORY/jars -name '*.jar' -printf '%p:' | sed 's/:$//')
