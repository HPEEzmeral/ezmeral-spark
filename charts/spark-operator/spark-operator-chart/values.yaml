# Default values for spark-operator.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# replicaCount -- Desired number of pods, leaderElection will be enabled
# if this is greater than 1
replicaCount: 1

image:
  # -- Image repository
  baseRepository: gcr.io/mapr-252711
  # -- Image Name
  imageName: spark-operator-1.3.8
  # -- Image tag
  tag: "1.3.8.7-hpe"
  # -- Image pull policy
  pullPolicy: IfNotPresent
  # -- Image name for autotix
  autotixImageName: autoticketgen-2.1.2
  # -- Image tag for autotix
  autotixTag: "202404040654"

# -- Image pull secrets
imagePullSecrets:
  - name: "imagepull"

# -- Whether to create a default pull secret
createDefaultPullSecret: true
# -- Default pull secret name to be created
defaultPullSecret: imagepull

# -- String to partially override `spark-operator.fullname` template (will maintain the release name)
nameOverride: ""

# -- String to override release name
fullnameOverride: ""

rbac:
  # -- Create and use `rbac` resources
  create: true

serviceAccounts:
  spark:
    # -- Create a service account for spark apps
    create: false
    # -- Optional name for the spark service account
    name: ""
  sparkoperator:
    # -- Create a service account for the operator
    create: true
    # -- Optional name for the operator service account
    name: ""
  autotix:
    # -- Create a service account for the autoticket generator
    create: true
    # -- Optional name for the autoticket generator service account
    name: ""

# -- Set this if running spark jobs in a different namespace than the operator
sparkJobNamespace: ""

# -- Operator concurrency, higher values might increase memory usage
controllerThreads: 10

# -- Operator resync interval. Note that the operator will respond to events (e.g. create, update)
# unrealted to this setting
resyncInterval: 30

# -- Ingress URL format
ingressUrlFormat: ""

# -- Set higher levels for more verbose logging
logLevel: 2

# podSecurityContext -- Pod security context
podSecurityContext: {}

# securityContext -- Operator container security context
securityContext: {}

webhook:
  # -- Enable webhook server
  enable: true
  # -- Webhook service port
  port: 8080
  # -- The webhook server will only operate on namespaces with this label, specified in the form key1=value1,key2=value2.
  # Empty string (default) will operate on all namespaces
  # If tenant integration is enabled - selector will be autogenerated to work with given particular tenant
  namespaceSelector: ""

autotix:
  # -- Enable autoticket generator
  enable: false
  # -- String autoticket-generator name
  autotixName: "autoticket-generator"
  # -- ports for autoticket generator service
  port: 443
  targetPort: 8443
  automountSAToken: true
  priorityClassName: "hpe-critical"
  rbac:
    # -- Create and use `rbac` resources
    create: true
  # resources -- Pod resource requests and limits
  resources:
    requests:
      memory: 1Gi
      cpu: 500m
  # Note: all ENV variables override value from 'configuration'
  # add env variable to add labels for observability -- default is set to true
#  configureLabel: true
  # add env variable to enable audit Logging
#  auditLoggingEnable: false
  # add env variable for auditLogging URL
  #auditLoggingURL: "http://ua-monitor-svc.monitoring.svc.cluster.local:3333/monitoring/v1/audit"
#  auditLoggingURL: ""
  # enable auto-setting 'SPARK_USER' env variables by autotix webhook
  # sssd values - nativesssd (using AD/LDAP), uaconfigmap (without AD/LDAP)
  autoSetSparkUserNameEnvs: true
  # labels for observability
  configuration:
    labeling:
      enabled: false
      labels:
#        "hpe-ezua/app": "spark"
#        "hpe-ezua/type": "app-service-user"
#        "sidecar.istio.io/inject": false
    auditing:
      enabled: false
#      url: "https://home.auditing-server.com:1234"
    lookup_user_svc:
      value: nativesssd

metrics:
  # -- Enable prometheus metric scraping
  enable: false
  # -- Metrics port
  port: 10254
  # -- Metrics port name
  portName: metrics
  # -- Metrics serving endpoint
  endpoint: /metrics
  # -- Metric prefix, will be added to all exported metrics
  prefix: ""
  # -- Metric labels. Operator will look up for values in SparkApp metadata.labels
  # -- Some labels will get values even if no match was found in app metadata labels. For such case the metrics label
  # -- value will be taken from app spec as shown below:
  # -- METRICS LABEL       APP SPEC VALUE
  # -- 'namespace'    =>  metadata.namespace
  # -- 'app_type'     =>  spec.type
  # -- 'app_name'     =>  metadata.name
  # -- 'app_version'  =>  spec.sparkVersion
  #
  # -- Example:
#  labels:
#  - "app_type"
#  - "app_name"
#  - "namespace"
#  - "user"
  labels: []

# -- Prometheus pod monitor for operator's pod.
podMonitor:
  # -- If enabled, a pod monitor for operator's pod will be submitted. Note that prometheus metrics should be enabled as well.
  enable: false
  # -- Pod monitor labels
  labels: {}
  # -- The label to use to retrieve the job name from
  jobLabel: spark-operator-podmonitor
  # -- Prometheus metrics endpoint properties. `metrics.portName` will be used as a port
  podMetricsEndpoint:
    scheme: http
    interval: 5s

# nodeSelector -- Node labels for pod assignment
nodeSelector: {}

# tolerations -- List of node taints to tolerate
tolerations: []

# affinity -- Affinity for pod assignment
affinity: {}

# podAnnotations -- Additional annotations to add to the pod
podAnnotations: {}

# resources -- Pod resource requests and limits
resources:
   limits:
     cpu: 1000m
     memory: 1024Mi
   requests:
     cpu: 1000m
     memory: 1024Mi

batchScheduler:
  # -- Enable batch scheduler for spark jobs scheduling. If enabled, users can specify batch scheduler name in spark application
  enable: false

resourceQuotaEnforcement:
  # -- Whether to enable the ResourceQuota enforcement for SparkApplication resources.
  # Requires the webhook to be enabled by setting `webhook.enable` to true.
  # Ref: https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/blob/master/docs/user-guide.md#enabling-resource-quota-enforcement.
  enable: false

leaderElection:
  # -- Leader election lock name.
  # Ref: https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/blob/master/docs/user-guide.md#enabling-leader-election-for-high-availability.
  lockName: "spark-operator-lock"
  # -- Optionally store the lock in another namespace. Defaults to operator's namespace
  lockNamespace: ""

istio:
  # -- When using `istio`, spark jobs need to run without a sidecar to properly terminate
  enabled: false

#owner References
ownerReference:
  overRide: false
  ownerReferences: {}

customLabels:
