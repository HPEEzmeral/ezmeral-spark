#
# Copyright 2017 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

apiVersion: "sparkoperator.hpe.com/v1beta2"
kind: SparkApplication
metadata:
  name: spark-pi-mapr
  namespace: test123
spec:
  sparkConf:
    # Note: If you are executing the application as a K8 user that MapR can verify,
    #       you do not need to specify a spark.mapr.user.secret
    #spark.mapr.user.secret: spark-user-secret
    # Note: You do not need to specify a spark.eventLog.dir
    #       it will be auto-generated with the pattern "maprfs:///apps/spark/<namespace>"
    spark.eventLog.dir: "s3a://apps/spark/test123"
    spark.eventLog.enabled: "true"
    spark.hadoop.fs.s3a.endpoint: "http://16.0.14.114:9000"
    spark.hadoop.fs.s3a.access.key: "AKIAIOSFODNN7EXAMPLE"
    spark.hadoop.fs.s3a.secret.key: "wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY"
    spark.hadoop.fs.s3a.path.style.access: "true"
    spark.hadoop.fs.s3a.impl: "org.apache.hadoop.fs.s3a.S3AFileSystem"
    ## Note: To customize truststore with AWS s3 uncomment these flags
    #spark.driver.extraJavaOptions: "-Dcom.amazonaws.sdk.disableCertChecking=true"
    #spark.executor.extraJavaOptions: "-Dcom.amazonaws.sdk.disableCertChecking=true"
  type: Scala
  sparkVersion: 3.3.1
  mode: cluster
  image: gcr.io/mapr-252711/spark-3.3.1:202304252229AN
  imagePullPolicy: Always
  imagePullSecrets:
    - imagepull
  mainClass: org.apache.spark.examples.SparkPi
  mainApplicationFile: "local:///opt/mapr/spark/spark-3.3.1/examples/jars/spark-examples_2.12-3.3.1.0-eep-910.jar"
  restartPolicy:
    type: Never
  driver:
    cores: 1
    coreLimit: "1000m"
    memory: "512m"
    labels:
      version: 3.3.1
    # Note: You do not need to specify a serviceAccount
    #       it will be auto-generated referencing the pre-existing "hpe-<namespace>"
  #    serviceAccount: hpe-spark-operator
  executor:
    cores: 1
    coreLimit: "1000m"
    instances: 1
    memory: "512m"
    labels:
      version: 3.3.1
